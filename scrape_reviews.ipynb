{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selectorlib import Extractor\n",
    "import requests \n",
    "import json \n",
    "from time import sleep\n",
    "import csv\n",
    "from dateutil import parser as dateparser\n",
    "import pandas as pd\n",
    "import num2words as n2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Extractor by reading from the YAML file\n",
    "e = Extractor.from_yaml_file('selectors.yml')\n",
    "\n",
    "def scrape(url):    \n",
    "    headers = {\n",
    "        'authority': 'www.amazon.com',\n",
    "        'pragma': 'no-cache',\n",
    "        'cache-control': 'no-cache',\n",
    "        'dnt': '1',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'sec-fetch-site': 'none',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8'\n",
    "    }\n",
    "\n",
    "    # Download the page using requests\n",
    "    print(\"Downloading %s\"%url)\n",
    "    r = requests.get(url, headers=headers)\n",
    "    # Simple check to check if page was blocked (Usually 503)\n",
    "    if r.status_code > 500:\n",
    "        if \"To discuss automated access to Amazon data please contact\" in r.text:\n",
    "            print(\"Page %s was blocked by Amazon. Please try using better proxies\\n\"%url)\n",
    "        else:\n",
    "            print(\"Page %s must have been blocked by Amazon as the status code was %d\"%(url,r.status_code))\n",
    "        return None\n",
    "    # Pass the HTML of the page and create \n",
    "    return e.extract(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "webpage: https://www.amazon.com/product-reviews/B07NVTGRVZ/ref=cm_cr_getr_d_paging_btm_prev_1&filterByStar=one_star&pageNumber=1\n",
      "https://www.amazon.com/product-reviews/B07NVTGRVZ/ref=cm_cr_getr_d_paging_btm_next_\n",
      "Downloading https://www.amazon.com/product-reviews/B07NVTGRVZ/ref=cm_cr_getr_d_paging_btm_next_1?filterByStar=one_star&pageNumber=1\n",
      "Downloading https://www.amazon.com/product-reviews/B07NVTGRVZ/ref=cm_cr_getr_d_paging_btm_next_2?filterByStar=one_star&pageNumber=2\n",
      "Downloading https://www.amazon.com/product-reviews/B07NVTGRVZ/ref=cm_cr_getr_d_paging_btm_next_3?filterByStar=one_star&pageNumber=3\n",
      "Downloading https://www.amazon.com/product-reviews/B07NVTGRVZ/ref=cm_cr_getr_d_paging_btm_next_4?filterByStar=one_star&pageNumber=4\n",
      "Downloading https://www.amazon.com/product-reviews/B07NVTGRVZ/ref=cm_cr_getr_d_paging_btm_next_5?filterByStar=one_star&pageNumber=5\n",
      "Downloading https://www.amazon.com/product-reviews/B07NVTGRVZ/ref=cm_cr_getr_d_paging_btm_next_6?filterByStar=one_star&pageNumber=6\n",
      "Downloading https://www.amazon.com/product-reviews/B07NVTGRVZ/ref=cm_cr_getr_d_paging_btm_next_7?filterByStar=one_star&pageNumber=7\n"
     ]
    }
   ],
   "source": [
    "with open(\"urls.txt\",'r') as urllist, open('data_reviews.csv','w') as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames=[\"title\",\"content\",\"country\",\"date\",\"images\",\"verified\",\"author\",\"rating\",\"product\",\"variant\",\"url\"],quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    plur=''\n",
    "    for url_in in urllist.readlines():\n",
    "        print('webpage:',url_in)\n",
    "        url_part_1=url_in.split('&filterByStar=')[0].replace('_prev_','_next_')[:-1]\n",
    "        print(url_part_1)\n",
    "#        data = scrape(url_in+'&pageNumber='+str(page_n))\n",
    "        for star in range(1,6):\n",
    "            page_n=1\n",
    "            while page_n<200:\n",
    "                url=url_part_1+str(page_n)+'?filterByStar='+n2w.num2words(star)+'_star&pageNumber='+str(page_n)\n",
    "                page_n+=1\n",
    "                data = scrape(url)\n",
    "            #    print(pd.DataFrame(data['reviews']).info())\n",
    "                try:\n",
    "                    for r in data['reviews']:\n",
    "                #        print(r['rating'])\n",
    "                        r[\"product\"] = data[\"product_title\"]\n",
    "                        r['url'] = url\n",
    "                        try:\n",
    "                            if 'verified' in r:\n",
    "                                if 'Verified Purchase' in r['verified']:\n",
    "                #                    print(r['verified'])\n",
    "                                    r['verified'] = 'Yes'\n",
    "                                else:\n",
    "                                    r['verified'] = 'No'\n",
    "                        except TypeError:\n",
    "                            r['verified'] = 'No'\n",
    "                        try:\n",
    "                            r['rating'] = r['rating'].split(' out of')[0]\n",
    "                        except AttributeError:\n",
    "            #                print('rating error')\n",
    "            #                print(data)\n",
    "                            r['rating'] = None\n",
    "                        r['country']=((r['date'].split('on ')[0]).split('Reviewed in ')[1])[:-1].replace('the United States','USA')\n",
    "                        date_posted = r['date'].split('on ')[-1]\n",
    "                        if r['images']:\n",
    "                            r['images'] = \"\\n\".join(r['images'])\n",
    "                        r['date'] = dateparser.parse(date_posted).strftime('%d %b %Y')\n",
    "                        writer.writerow(r)\n",
    "                        # sleep(5)\n",
    "                except TypeError:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
